# train.py (å„ªåŒ–å¾Œ)

import os
from PIL import Image
import torch
from torch.utils.data import Dataset
from transformers import SegformerForSemanticSegmentation, SegformerImageProcessor, Trainer, TrainingArguments, SegformerConfig # <--- ADDED SegformerConfig
import json
import numpy as np

# =====================================================================================
# >>>>> 1. è¨­å®šæ‚¨çš„å°ˆæ¡ˆé…ç½® <<<<<
# =====================================================================================

# --- æ•¸æ“šé›†è·¯å¾‘ ---
TRAIN_DIR = "./data/train"
VAL_DIR = "./data/validation"

# --- æ¨¡å‹é…ç½® ---
PRETRAINED_MODEL = "nvidia/segformer-b0-finetuned-ade-512-512"

# --- æ‚¨çš„é¡åˆ¥å®šç¾© (!! éå¸¸é‡è¦ !!) ---
ID2LABEL = {
    0: "background",
    1: "wall",
    2: "door",
    3: "roof",
    4: "floor",
    5: "outside",
    6: "cargo",
    7: "person",
}

# --- è¨“ç·´è¼¸å‡º ---
OUTPUT_DIR = "./segformer-finetuned-custom"

# --- æ¨¡å‹ä¿å­˜æ ¼å¼é…ç½® ---
SAVE_AS_PT = True          # æ˜¯å¦ä¿å­˜ç‚º PyTorch .pt æ ¼å¼
SAVE_AS_ONNX = True        # æ˜¯å¦ä¿å­˜ç‚º ONNX æ ¼å¼
SAVE_TRANSFORMERS = False   # æ˜¯å¦ä¿å­˜ Transformers æ ¼å¼ï¼ˆåŸæœ‰æ–¹å¼ï¼‰


# =====================================================================================
# >>>>> 2. å»ºç«‹è‡ªè¨‚ Dataset <<<<<
# =====================================================================================

class CustomSegmentationDataset(Dataset):
    """è‡ªè¨‚å½±åƒåˆ†å‰²æ•¸æ“šé›†"""
    def __init__(self, root_dir, image_processor):
        self.root_dir = root_dir
        self.image_processor = image_processor
        self.image_dir = os.path.join(root_dir, "images")
        self.mask_dir = os.path.join(root_dir, "masks")
        
        image_files = sorted(os.listdir(self.image_dir))
        mask_files = sorted(os.listdir(self.mask_dir))
        
        self.images = [f for f in image_files if f.endswith(('.jpg', '.png'))]
        self.masks = [f for f in mask_files if f.endswith('.png')]
        
        assert len(self.images) == len(self.masks), "å½±åƒå’Œé®ç½©çš„æ•¸é‡ä¸åŒ¹é…ï¼"

    def __len__(self):
        return len(self.images)

    def __getitem__(self, idx):
        image = Image.open(os.path.join(self.image_dir, self.images[idx])).convert("RGB")
        mask = Image.open(os.path.join(self.mask_dir, self.masks[idx]))
        
        inputs = self.image_processor(images=image, segmentation_maps=mask, return_tensors="pt")
        
        pixel_values = inputs["pixel_values"].squeeze(0)
        labels = inputs["labels"].squeeze(0)
        
        return {"pixel_values": pixel_values, "labels": labels}

# =====================================================================================
# >>>>> 2.5. æ¨¡å‹ä¿å­˜åŠŸèƒ½ <<<<<
# =====================================================================================

def save_model_as_pt(model, image_processor, output_path, config_dict):
    """
    å°‡æ¨¡å‹ä¿å­˜ç‚º PyTorch .pt æ ¼å¼
    """
    print(f"æ­£åœ¨ä¿å­˜ PyTorch .pt æ ¼å¼æ¨¡å‹åˆ°: {output_path}")
    
    # æº–å‚™ä¿å­˜çš„å…§å®¹
    model_state = {
        'model_state_dict': model.state_dict(),
        'config': config_dict,
        'model_config': model.config.to_dict(),
        'processor_config': image_processor.to_dict(), # <--- MODIFIED: ç¢ºä¿ processor config è¢«å„²å­˜
        'id2label': config_dict['id2label'],
        'label2id': config_dict['label2id'],
        'num_classes': len(config_dict['id2label'])
    }
    
    torch.save(model_state, output_path)
    print(f"âœ… PyTorch æ¨¡å‹å·²ä¿å­˜ç‚º: {output_path}")

def save_model_as_onnx(model, image_processor, output_path, config_dict):
    """
    å°‡æ¨¡å‹ä¿å­˜ç‚º ONNX æ ¼å¼
    """
    try:
        print(f"æ­£åœ¨ä¿å­˜ ONNX æ ¼å¼æ¨¡å‹åˆ°: {output_path}")
        
        model.eval()
        
        # <--- MODIFIED START: ç¢ºä¿ dummy input å’Œ model åœ¨åŒä¸€å€‹ device ---
        device = next(model.parameters()).device
        print(f"ONNX å°å‡º: æ¨¡å‹ä½æ–¼ {device} è¨­å‚™")
        
        # å‰µå»ºä¸€å€‹èˆ‡æ¨¡å‹åœ¨ç›¸åŒè¨­å‚™ä¸Šçš„ç¤ºä¾‹è¼¸å…¥
        dummy_input = torch.randn(1, 3, 512, 512, device=device)
        # <--- MODIFIED END ---
        
        # å°å‡ºç‚º ONNX
        torch.onnx.export(
            model,
            dummy_input,
            output_path,
            export_params=True,
            opset_version=11,
            do_constant_folding=True,
            input_names=['pixel_values'],
            output_names=['logits'],
            dynamic_axes={
                'pixel_values': {0: 'batch_size', 2: 'height', 3: 'width'},
                'logits': {0: 'batch_size', 2: 'height', 3: 'width'}
            }
        )
        
        # ä¿å­˜é…ç½®ä¿¡æ¯åˆ°åŒåçš„ JSON æ–‡ä»¶
        config_path = output_path.replace('.onnx', '_config.json')
        with open(config_path, 'w', encoding='utf-8') as f:
            json.dump(config_dict, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… ONNX æ¨¡å‹å·²ä¿å­˜ç‚º: {output_path}")
        print(f"âœ… é…ç½®æ–‡ä»¶å·²ä¿å­˜ç‚º: {config_path}")
        
    except Exception as e:
        print(f"âŒ ONNX å°å‡ºå¤±æ•—: {str(e)}")
        print("é€™å¯èƒ½æ˜¯å› ç‚ºç¼ºå°‘ ONNX ç›¸é—œä¾è³´ï¼Œè«‹å®‰è£: pip install onnx onnxruntime")

def create_standalone_model_config(model, image_processor):
    """
    å‰µå»ºç¨ç«‹æ¨¡å‹çš„é…ç½®ä¿¡æ¯
    """
    config = {
        'id2label': model.config.id2label,
        'label2id': model.config.label2id,
        'num_classes': model.config.num_labels,
        'model_type': 'segformer',
        'architecture': model.config.architectures[0] if hasattr(model.config, 'architectures') else 'SegformerForSemanticSegmentation',
        'image_size': getattr(model.config, 'image_size', 512),
        'num_channels': getattr(model.config, 'num_channels', 3),
        'num_encoder_blocks': getattr(model.config, 'num_encoder_blocks', 4),
        'depths': getattr(model.config, 'depths', [2, 2, 2, 2]),
        'sr_ratios': getattr(model.config, 'sr_ratios', [8, 4, 2, 1]),
        'hidden_sizes': getattr(model.config, 'hidden_sizes', [32, 64, 160, 256]),
        'decoder_hidden_size': getattr(model.config, 'decoder_hidden_size', 256),
        'patch_sizes': getattr(model.config, 'patch_sizes', [7, 3, 3, 3]),
        'strides': getattr(model.config, 'strides', [4, 2, 2, 2]),
        'num_attention_heads': getattr(model.config, 'num_attention_heads', [1, 2, 5, 8]),
        'mlp_ratios': getattr(model.config, 'mlp_ratios', [4, 4, 4, 4]),
        'hidden_act': getattr(model.config, 'hidden_act', 'gelu'),
        'hidden_dropout_prob': getattr(model.config, 'hidden_dropout_prob', 0.0),
        'attention_probs_dropout_prob': getattr(model.config, 'attention_probs_dropout_prob', 0.0),
        'classifier_dropout_prob': getattr(model.config, 'classifier_dropout_prob', 0.1),
        'initializer_range': getattr(model.config, 'initializer_range', 0.02),
        'drop_path_rate': getattr(model.config, 'drop_path_rate', 0.1),
        'layer_norm_eps': getattr(model.config, 'layer_norm_eps', 1e-6),
        'semantic_loss_ignore_index': getattr(model.config, 'semantic_loss_ignore_index', 255),
    }
    
    # æ·»åŠ è™•ç†å™¨ç›¸é—œé…ç½®
    processor_dict = image_processor.to_dict()
    config.update({f"processor_{key}": value for key, value in processor_dict.items()})
    
    return config

# =====================================================================================
# >>>>> 3. ä¸»è¨“ç·´æµç¨‹ <<<<<
# =====================================================================================

def main():
    print("===== é–‹å§‹è¨­å®šæ¨¡å‹ =====")
    
    label2id = {v: k for k, v in ID2LABEL.items()}
    num_classes = len(ID2LABEL)

    image_processor = SegformerImageProcessor.from_pretrained(PRETRAINED_MODEL, reduce_labels=False)
    
    model = SegformerForSemanticSegmentation.from_pretrained(
        PRETRAINED_MODEL,
        num_labels=num_classes,
        id2label=ID2LABEL,
        label2id=label2id,
        ignore_mismatched_sizes=True
    )
    print("===== æ¨¡å‹è¨­å®šå®Œæˆ =====")
    
    print("\n===== æº–å‚™æ•¸æ“šé›† =====")
    train_dataset = CustomSegmentationDataset(root_dir=TRAIN_DIR, image_processor=image_processor)
    val_dataset = CustomSegmentationDataset(root_dir=VAL_DIR, image_processor=image_processor)
    print(f"è¨“ç·´é›†æ•¸é‡: {len(train_dataset)}, é©—è­‰é›†æ•¸é‡: {len(val_dataset)}")
    print("===== æ•¸æ“šé›†æº–å‚™å®Œæˆ =====")

    training_args = TrainingArguments(
        output_dir=OUTPUT_DIR,
        learning_rate=6e-5,
        num_train_epochs=50,
        per_device_train_batch_size=4,
        per_device_eval_batch_size=2,
        save_total_limit=3,
        evaluation_strategy="steps",
        eval_steps=200,
        save_steps=200,
        logging_steps=50,
        remove_unused_columns=False,
        load_best_model_at_end=True,
    )

    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=val_dataset,
    )

    print("\n===== âœ¨âœ¨ é–‹å§‹è¨“ç·´ï¼ âœ¨âœ¨ =====")
    trainer.train()
    print("===== è¨“ç·´å®Œæˆï¼ =====")

    print("\n===== ä¿å­˜æ¨¡å‹ =====")
    
    # å‰µå»ºè¼¸å‡ºç›®éŒ„
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    
    # å‰µå»ºæ¨¡å‹é…ç½®
    model_config_for_saving = create_standalone_model_config(model, image_processor)
    
    # 1. ä¿å­˜ Transformers æ ¼å¼ï¼ˆåŸæœ‰æ–¹å¼ï¼‰
    if SAVE_TRANSFORMERS:
        final_model_path = os.path.join(OUTPUT_DIR, "final")
        trainer.save_model(final_model_path)
        image_processor.save_pretrained(final_model_path)
        print(f"âœ… Transformers æ ¼å¼æ¨¡å‹å·²å„²å­˜è‡³: {final_model_path}")
    
    # 2. ä¿å­˜ç‚º PyTorch .pt æ ¼å¼
    if SAVE_AS_PT:
        pt_model_path = os.path.join(OUTPUT_DIR, "segformer_model.pt")
        save_model_as_pt(model, image_processor, pt_model_path, model_config_for_saving)
    
    # 3. ä¿å­˜ç‚º ONNX æ ¼å¼
    if SAVE_AS_ONNX:
        onnx_model_path = os.path.join(OUTPUT_DIR, "segformer_model.onnx")
        save_model_as_onnx(model, image_processor, onnx_model_path, model_config_for_saving)
    
    print(f"\nğŸ‰ æ‰€æœ‰æ¨¡å‹æ ¼å¼å·²ä¿å­˜å®Œæˆï¼")
    print(f"è¼¸å‡ºç›®éŒ„: {OUTPUT_DIR}")
    if SAVE_TRANSFORMERS:
        print(f"  - Transformers æ ¼å¼: {os.path.join(OUTPUT_DIR, 'final')}")
    if SAVE_AS_PT:
        print(f"  - PyTorch .pt æ ¼å¼: {os.path.join(OUTPUT_DIR, 'segformer_model.pt')}")
    if SAVE_AS_ONNX:
        print(f"  - ONNX æ ¼å¼: {os.path.join(OUTPUT_DIR, 'segformer_model.onnx')}")
        print(f"  - ONNX é…ç½®: {os.path.join(OUTPUT_DIR, 'segformer_model_config.json')}")

# =====================================================================================
# >>>>> 4. æ¨¡å‹è¼‰å…¥ç¯„ä¾‹ (å„ªåŒ–å¾Œ) <<<<<
# =====================================================================================
def load_pt_model(model_path, device='cpu'):
    """
    è¼‰å…¥ PyTorch .pt æ ¼å¼çš„æ¨¡å‹ (å„ªåŒ–å¾Œï¼Œå¯¦ç¾å®Œå…¨é›¢ç·šè¼‰å…¥)
    
    ä½¿ç”¨ç¯„ä¾‹:
    model, processor, config = load_pt_model('segformer_model.pt')
    """
    print(f"æ­£åœ¨è¼‰å…¥ PyTorch æ¨¡å‹: {model_path}")
    
    # <--- MODIFIED START: æ¡ç”¨å®Œå…¨é›¢ç·šçš„è¼‰å…¥æ–¹å¼ ---
    checkpoint = torch.load(model_path, map_location=device)
    
    # 1. å¾å„²å­˜çš„ model_config é‡å»ºæ¨¡å‹çµæ§‹
    # é€™ä½¿å¾—æ¨¡å‹è¼‰å…¥ä¸å†éœ€è¦ç¶²è·¯æˆ– PRETRAINED_MODEL å­—ä¸²
    model_config_dict = checkpoint['model_config']
    config = SegformerConfig(**model_config_dict)
    model = SegformerForSemanticSegmentation(config)

    # 2. è¼‰å…¥è¨“ç·´å¥½çš„æ¬Šé‡
    model.load_state_dict(checkpoint['model_state_dict'])
    model.to(device)
    model.eval()
    
    print(f"âœ… æ¨¡å‹è¼‰å…¥æˆåŠŸï¼Œè¨­å‚™: {device}")
    
    # 3. å¾å„²å­˜çš„ processor_config é‡å»º ImageProcessor
    image_processor = SegformerImageProcessor.from_dict(checkpoint['processor_config'])
    
    print(f"âœ… åœ–åƒè™•ç†å™¨é‡å»ºæˆåŠŸ")

    return model, image_processor, checkpoint['config']
    # <--- MODIFIED END ---

if __name__ == "__main__":
    main()